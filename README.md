# Week #1
## Study
### [网易云课堂 吴恩达 卷积神经网络](https://mooc.study.163.com/learn/2001281004?tid=2001392030#/learn/content)
#### 2.5 网络中的网络以及1x1卷积
1. 1x1卷积也称为`Network in network`，因为每一次卷积相当于一个全连接层。
2. 1x1卷积可以在保持输入层长宽的基础上改变信道的数量



#### 2.6 谷歌Inception网络简介
Inception网络代替人工确定卷积层中的卷积核类型，确定是否需要创建卷积层或池化层

通过1x1卷积构建“瓶颈层”可降低运算量

#### 2.7 Inception网络

初始的激活层分别经过5x5卷积核、3X3卷积、1x1卷积、最大值池化（不改变长宽）后按信道连接在一起，即组成一个Inception模块。

论文：[[Szegedy et al.,2014, Going Deeper with Convolutions]](https://arxiv.org/abs/1409.4842)

#### 2.8 使用开源的实现方案
如题

#### 2.9 迁移学习
做计算机视觉项目时，可以下载开源项目，利用他人已经训练好的网络结构的权重，将之作为预训练，然后转换到自己感兴趣的任务上。

若训练数据少，可冻结之前的隐层，只改动最后的Softmax层。

训练数据越多，可适当改动之前的隐层

总之，可用下载的权重代替随机初始化的权重。

#### 2.10 数据扩充
计算机视觉的一个问题：数据不够

1. 垂直镜像对称
2. 随机修剪（不完美）
3. 旋转、局部扭曲（不推荐）
4. 色彩转换，修改RGB值（可使用PCA颜色增强算法）

数据的扩充和训练过程可并行执行

#### 2.11 计算机视觉现状

数据过少，从事的更多是手工工程（精心设计特征、网络结构等）

在benchmarks训练和竞赛上的技巧
1. 综合模型，结果取平均值
2. Multi-crop，如10-crop，即将一张图剪切出10份，增加数据量

### 斯坦福CS231N课程

#### 2.1 图像分类 - 数据驱动方法
 关于K近邻算法，要防止过拟合现象的产生，K需要适当取大于1的数。
 K近邻算法的缺点在于训练快而预测慢，与实际需求冲突

#### 2.2 图像分类 - K最近邻算法
L1(Manhattan)距离$$d_1(I_1,I_2)=\sum_p|I_1^p-I_2^p|$$

L2(Euclidean)距离$$d_1(I_1,I_2)=\sqrt{\sum_p(I_1^p-I_2^p)^2}$$

将数据分为`训练集`、`验证集`、`测试集`是较好的**寻找最佳超参数**的方法。
当数据较小时，可用**交叉验证**的方法，将数据集平分几份，每份轮流作验证集

K近邻算法的缺点：
1. 训练快，预测慢。
2. L1和L2距离算法不适合表示图像之间的相似度。
3. 需要训练点分布得尽可能密集，这样导致训练数据成倍增加，尤其在高维时所需的数据很大。




## Programs

## Summary
