{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./img/',exist_ok=True)\n",
    "\n",
    "IMAGE_URL=\"https://morvanzhou.github.io/static/img/description/learning_step_flowchart.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./img/image1.png', <http.client.HTTPMessage at 0x175ce86e080>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "urlretrieve(IMAGE_URL,'./img/image1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(IMAGE_URL)\n",
    "with open('./img/image2.png','wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分块下载\n",
    "适合大文件的下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r=requests.get(IMAGE_URL,stream=True)\n",
    "\n",
    "with open('./img/image3.png','wb') as f:\n",
    "    for chunk in r.iter_content(chunk_size=32): #32 Bytes\n",
    "        f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下载美图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "\n",
    "URL='http://www.nationalgeographic.com.cn/animals/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html=requests.get(URL).text\n",
    "soup=BeautifulSoup(html,features='lxml')\n",
    "img_ul=soup.find_all('ul',{'class':'img_list'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./img/',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20181121122540273.jpg\n",
      "Saved 20181114043840168.jpg\n",
      "Saved 20181105025539322.jpg\n",
      "Saved 20181029110713949.jpg\n",
      "Saved 20181024034038271.jpg\n",
      "Saved 20181023014700420.jpg\n"
     ]
    }
   ],
   "source": [
    "for ul in img_ul:\n",
    "    imgs=ul.find_all('img')\n",
    "    for img in imgs:\n",
    "        url=img['src']\n",
    "        r=requests.get(url,stream=True)\n",
    "        image_name=url.split('/')[-1]\n",
    "        with open('./img/{}'.format(image_name),'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=128):\n",
    "                f.write(chunk)\n",
    "        print('Saved {}'.format(image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习：爬取豆瓣电影海报"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import webbrowser\n",
    "\n",
    "URL=r'https://movie.douban.com/explore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "html=requests.get(URL).text\n",
    "soup=BeautifulSoup(html,features='lxml')\n",
    "links=soup.find_all('a',)\n",
    "#print(poster_div)\n",
    "#webbrowser.open(URL)\n",
    "for link in links:\n",
    "    url=link['href']\n",
    "    #print(url)\n",
    "    \n",
    "import os\n",
    "os.makedirs('./links/',exist_ok=True)\n",
    "with open('./links/links.txt','w') as f:\n",
    "    for link in links:\n",
    "        url=link['href']\n",
    "        f.writelines(url+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./movie_posters/',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for div in poster_div:\n",
    "    imgs=div.find_all('img')\n",
    "    for img in imgs:\n",
    "        url=img['src']\n",
    "        img_name=url.split('/')[-1]\n",
    "        r=requests.get(url,stream=True)\n",
    "        with open('/movie_posters/{}'.format(img_name),'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=256):\n",
    "                f.write(chunk)\n",
    "        print('Saved {}'.format(img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
