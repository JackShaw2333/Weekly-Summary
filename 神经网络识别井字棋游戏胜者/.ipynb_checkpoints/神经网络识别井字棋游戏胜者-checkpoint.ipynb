{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Recognizing Tic-Tac-Toe Winners with Neural Networks 用神经网络识别井字棋游戏的胜者](https://www.kdnuggets.com/2017/09/neural-networks-tic-tac-toe-keras.html)\n",
    "[数据集](https://archive.ics.uci.edu/ml/datasets/Tic-Tac-Toe+Endgame)是由958个可能的井字棋游戏结局组成的，每个数据有9个表示井字棋棋盘的9个格子状态的变量，第10个变量表示该数据描述的结局对于玩家X而言的胜负情况。\n",
    "\n",
    "由于一局井字棋有255,168种可能的棋局方式，所以很难通过设置规则的方式布置棋局。\n",
    "\n",
    "以下是对数据的一种描述。\n",
    ">1. top-left-square: {x,o,b} \n",
    ">2. top-middle-square: {x,o,b} \n",
    ">3. top-right-square: {x,o,b} \n",
    ">4. middle-left-square: {x,o,b} \n",
    ">5. middle-middle-square: {x,o,b} \n",
    ">6. middle-right-square: {x,o,b} \n",
    ">7. bottom-left-square: {x,o,b} \n",
    ">8. bottom-middle-square: {x,o,b} \n",
    ">9. bottom-right-square: {x,o,b} \n",
    ">10. Class: {positive,negative}\n",
    "\n",
    "\n",
    "每个格子可以被标为x,o或者b(空)，positive或者negative表明玩家X的胜负情况。\n",
    "\n",
    "#### 准备工作\n",
    "以下是我们在构建神经网络前对数据进行的一些处理工作。\n",
    "\n",
    "- **将类别变量编码为数字。**我们将{x, o ,b}转化为{0, 1, 2}， sklearn中preprocessing的[`LabelEncoder class`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)可以实现这个功能。\n",
    "- **用`One-hot`编码所有独立的类别变量。**`One-hot`编码将独立变量进一步表示为向量，因为仅仅将变量转化为数字并不能保证它们两两之间的欧式距离相等。这里我们可以用含2个元素的向量表示3个类别。sklearn中的[`OneHotEncoder class`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)可实现这个功能。\n",
    "- **避免虚拟变量陷阱**\n",
    "（这个概念不是特别理解）教程原文如下。\n",
    ">Remove every third column to avoid dummy variable trap - As the one-hot encoding process in Scikit-learn creates as many columns for each variables as there are possible options (as per the dataset), one column needs to be removed in order to avoid what is referred to as the dummy variable trap. This is so to avoid redundant data which could bias results. In our case, each square has 3 possible options (27 columns), which can be expressed with 2 'bit' columns (I leave this to you to confirm), and so we remove every third column from the newly formed dataset (leaving us with 18).\n",
    "\n",
    "  PS 虚拟变量陷阱：若有m个定性变量，则只在模型中引入m-1个虚拟变量，若引入m个虚拟变量，会导致模型解释变量间出现完全共线性的情况。\n",
    "\n",
    "  比如这里我们要表示每个格子的状态，下面这种编码方式不合适，会导致完全共线性。\n",
    "  >X: [1, 0 ,0]\n",
    "\n",
    "  >O: [0, 1 ,0]\n",
    "\n",
    "  >b: [0, 0, 1]\n",
    "\n",
    "  所以按原文，每过两列需要删除一列，这样编码就变成了\n",
    "  >X: [1, 0] \n",
    "\n",
    "  >O: [0, 1]\n",
    "\n",
    "  >b: [0, 0]\n",
    "\n",
    "- **编码目标类别变量**，如{positive, negative}→{0,1}\n",
    "\n",
    "- **训练集/测试集分割**，我们将20%的数据作为测试集。\n",
    "\n",
    "#### 神经网络\n",
    "我们一共有18个作为输入的变量，做2元分类\n",
    "- **输入单元**。因为有18个独立变量，所以我们需要18个输入神经元。\n",
    "\n",
    "- **隐藏层**。一个简单的决定每个隐藏层神经元个数的方法：把独立变量和输出变量的个数加起来再除以2。这里共19个变量，我们向下取整，每层设9个神经单元。关于隐藏层层数的设置，最好的方法是先设较小的层数，然后不断增加层数，直到网络的表现不再随层数提高为止。这里我们先设2层隐藏层。\n",
    "\n",
    "- **激活函数**。惯例：隐藏层默认使用`ReLU`函数，而2元分类的输出层使用`sigmoid`函数。这里我们按照这个惯例搭建网络。\n",
    "\n",
    "- **优化器**。我们使用Adam优化器\n",
    "\n",
    "- **损失函数**。我们使用二分类交叉熵损失函数\n",
    "\n",
    "- **权重初始化**。我们设定随机数作为初始权重。\n",
    "\n",
    "网络如下：\n",
    "1. dense_1_input: InputLayer\n",
    "input: (None, 18)\n",
    "output: (None, 18)\n",
    "2. dense_1: Dense\n",
    "input: (None, 18)\n",
    "output: (None, 9)\n",
    "3. dense_2: Dense\n",
    "input: (None, 9)\n",
    "output: (None, 9)\n",
    "4. dense_3: Dense\n",
    "input: (None, 9)\n",
    "output: (None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import preprocessing as ppc\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    data=[]\n",
    "    with open(file_name) as txt_data:\n",
    "        lines=txt_data.readlines()\n",
    "        for line in lines:\n",
    "            #.strip()默认去除行首尾空格\n",
    "            line=line.strip().split(',') \n",
    "            data.append(line)\n",
    "        return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    feature=[]\n",
    "    label=[]\n",
    "    for i in range(len(dataset)):\n",
    "        feature.append([data for data in dataset[i][:-1]])\n",
    "        label.append(dataset[i][-1])\n",
    "    return np.array(feature),np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data.txt'\n",
    "data = load_data(path)\n",
    "feature,label=data[:,:-1],data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备工作\n",
    "以下是我们在构建神经网络前对数据进行的一些处理工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**将类别变量编码为数字。**我们将{x, o ,b}转化为{0, 1, 2}， sklearn中preprocessing的[`LabelEncoder class`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)可以实现这个功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2' '2' '2' '2' '1' '1' '2' '1' '1']\n",
      " ['2' '2' '2' '2' '1' '1' '1' '2' '1']\n",
      " ['2' '2' '2' '2' '1' '1' '1' '1' '2']]\n",
      "['1' '1' '1']\n"
     ]
    }
   ],
   "source": [
    "#类别变量编码为数字\n",
    "label_encoder = ppc.LabelEncoder()\n",
    "label_encoder.fit(['x','o','b'])\n",
    "\n",
    "for i in range(len(feature)):\n",
    "    feature[i]=label_encoder.transform(feature[i])\n",
    "    \n",
    "for i in range(len(label)):\n",
    "    label[i]=1 if label[i]=='positive' else 0\n",
    "\n",
    "print(feature[:3])\n",
    "print(label[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**用`One-hot`编码所有独立的类别变量。**`One-hot`编码将独立变量进一步表示为向量，因为仅仅将变量转化为数字并不能保证它们两两之间的欧式距离相等。这里我们可以用含2个元素的向量表示3个类别。sklearn中的[`OneHotEncoder class`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)可实现这个功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "[0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "onehot_enc=ppc.OneHotEncoder(handle_unknown='ignore')\n",
    "onehot_enc.fit(feature)\n",
    "feature=onehot_enc.transform(feature).toarray()\n",
    "\n",
    "new_feature=[]\n",
    "\n",
    "print(feature.shape[1])\n",
    "columns=feature.shape[1]\n",
    "print(feature[0])\n",
    "\n",
    "for i in range(columns):\n",
    "    if (i+1)%3 != 0:\n",
    "        new_feature.append(feature.T[i])\n",
    "        \n",
    "new_feature=np.array(new_feature)\n",
    "feature=new_feature.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 避免虚拟变量陷阱 **\n",
    "（这个概念不是特别理解）教程原文如下。\n",
    ">Remove every third column to avoid dummy variable trap - As the one-hot encoding process in Scikit-learn creates as many columns for each variables as there are possible options (as per the dataset), one column needs to be removed in order to avoid what is referred to as the dummy variable trap. This is so to avoid redundant data which could bias results. In our case, each square has 3 possible options (27 columns), which can be expressed with 2 'bit' columns (I leave this to you to confirm), and so we remove every third column from the newly formed dataset (leaving us with 18).\n",
    "\n",
    "  PS 虚拟变量陷阱：若有m个定性变量，则只在模型中引入m-1个虚拟变量，若引入m个虚拟变量，会导致模型解释变量间出现完全共线性的情况。\n",
    "\n",
    "  比如这里我们要表示每个格子的状态，下面这种编码方式不合适，会导致完全共线性。\n",
    "  >X: [1, 0 ,0]\n",
    "\n",
    "  >O: [0, 1 ,0]\n",
    "\n",
    "  >b: [0, 0, 1]\n",
    "\n",
    "  所以按原文，每过两列需要删除一列，这样编码就变成了\n",
    "  >X: [1, 0] \n",
    "\n",
    "  >O: [0, 1]\n",
    "\n",
    "  >b: [0, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**编码目标类别变量**，如{positive, negative}→{0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(958, 18)\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "label=np_utils.to_categorical(label,2)\n",
    "\n",
    "#feature=feature.astype('float32')\n",
    "#label=label.astype('float32')\n",
    "print(feature.shape)\n",
    "#label=label[:,np.newaxis]\n",
    "print(label[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练集/测试集分割**，我们将20%的数据作为测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(feature,label,test_size=0.2,random_state=42)\n",
    "print(X_train[:3])\n",
    "print(y_train[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们一共有18个作为输入的变量，做2元分类\n",
    "- **输入单元**。因为有18个独立变量，所以我们需要18个输入神经元。\n",
    "\n",
    "- **隐藏层**。一个简单的决定每个隐藏层神经元个数的方法：把独立变量和输出变量的个数加起来再除以2。这里共19个变量，我们向下取整，每层设9个神经单元。关于隐藏层层数的设置，最好的方法是先设较小的层数，然后不断增加层数，直到网络的表现不再随层数提高为止。这里我们先设2层隐藏层。\n",
    "\n",
    "- **激活函数**。惯例：隐藏层默认使用`ReLU`函数，而2元分类的输出层使用`sigmoid`函数。这里我们按照这个惯例搭建网络。\n",
    "\n",
    "- **优化器**。我们使用Adam优化器\n",
    "\n",
    "- **损失函数**。我们使用二分类交叉熵损失函数\n",
    "\n",
    "- **权重初始化**。我们设定随机数作为初始权重。\n",
    "\n",
    "网络如下：\n",
    "1. dense_1_input: InputLayer\n",
    "input: (None, 18)\n",
    "output: (None, 18)\n",
    "2. dense_1: Dense\n",
    "input: (None, 18)\n",
    "output: (None, 9)\n",
    "3. dense_2: Dense\n",
    "input: (None, 9)\n",
    "output: (None, 9)\n",
    "4. dense_3: Dense\n",
    "input: (None, 9)\n",
    "output: (None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#模型搭建\n",
    "model = Sequential([\n",
    "    Dense(18,input_dim=18),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('relu'),\n",
    "    Dense(2),\n",
    "    Activation('sigmoid'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "766/766 [==============================] - 2s 3ms/step - loss: 0.6886 - acc: 0.5548\n",
      "Epoch 2/100\n",
      "766/766 [==============================] - 0s 316us/step - loss: 0.6387 - acc: 0.6540\n",
      "Epoch 3/100\n",
      "766/766 [==============================] - 0s 342us/step - loss: 0.6136 - acc: 0.6540\n",
      "Epoch 4/100\n",
      "766/766 [==============================] - 0s 224us/step - loss: 0.5948 - acc: 0.6540\n",
      "Epoch 5/100\n",
      "766/766 [==============================] - 0s 228us/step - loss: 0.5739 - acc: 0.6540\n",
      "Epoch 6/100\n",
      "766/766 [==============================] - 0s 229us/step - loss: 0.5524 - acc: 0.6906\n",
      "Epoch 7/100\n",
      "766/766 [==============================] - 0s 226us/step - loss: 0.5299 - acc: 0.7128\n",
      "Epoch 8/100\n",
      "766/766 [==============================] - 0s 197us/step - loss: 0.5073 - acc: 0.7298\n",
      "Epoch 9/100\n",
      "766/766 [==============================] - 0s 218us/step - loss: 0.4762 - acc: 0.7467\n",
      "Epoch 10/100\n",
      "766/766 [==============================] - 0s 197us/step - loss: 0.4464 - acc: 0.7702\n",
      "Epoch 11/100\n",
      "766/766 [==============================] - 0s 218us/step - loss: 0.4160 - acc: 0.7833\n",
      "Epoch 12/100\n",
      "766/766 [==============================] - 0s 217us/step - loss: 0.3881 - acc: 0.8107\n",
      "Epoch 13/100\n",
      "766/766 [==============================] - 0s 198us/step - loss: 0.3548 - acc: 0.8303\n",
      "Epoch 14/100\n",
      "766/766 [==============================] - 0s 231us/step - loss: 0.3288 - acc: 0.8538\n",
      "Epoch 15/100\n",
      "766/766 [==============================] - 0s 200us/step - loss: 0.3060 - acc: 0.8616\n",
      "Epoch 16/100\n",
      "766/766 [==============================] - 0s 220us/step - loss: 0.2848 - acc: 0.8825\n",
      "Epoch 17/100\n",
      "766/766 [==============================] - 0s 202us/step - loss: 0.2652 - acc: 0.8995\n",
      "Epoch 18/100\n",
      "766/766 [==============================] - 0s 187us/step - loss: 0.2494 - acc: 0.9112\n",
      "Epoch 19/100\n",
      "766/766 [==============================] - 0s 233us/step - loss: 0.2351 - acc: 0.9112\n",
      "Epoch 20/100\n",
      "766/766 [==============================] - 0s 203us/step - loss: 0.2200 - acc: 0.9217\n",
      "Epoch 21/100\n",
      "766/766 [==============================] - 0s 202us/step - loss: 0.2095 - acc: 0.9295\n",
      "Epoch 22/100\n",
      "766/766 [==============================] - 0s 216us/step - loss: 0.1962 - acc: 0.9399\n",
      "Epoch 23/100\n",
      "766/766 [==============================] - 0s 199us/step - loss: 0.1829 - acc: 0.9478\n",
      "Epoch 24/100\n",
      "766/766 [==============================] - 0s 181us/step - loss: 0.1711 - acc: 0.9478\n",
      "Epoch 25/100\n",
      "766/766 [==============================] - 0s 197us/step - loss: 0.1633 - acc: 0.9543\n",
      "Epoch 26/100\n",
      "766/766 [==============================] - 0s 205us/step - loss: 0.1534 - acc: 0.9517\n",
      "Epoch 27/100\n",
      "766/766 [==============================] - 0s 221us/step - loss: 0.1422 - acc: 0.9608\n",
      "Epoch 28/100\n",
      "766/766 [==============================] - 0s 236us/step - loss: 0.1371 - acc: 0.9621\n",
      "Epoch 29/100\n",
      "766/766 [==============================] - 0s 226us/step - loss: 0.1322 - acc: 0.9661\n",
      "Epoch 30/100\n",
      "766/766 [==============================] - 0s 243us/step - loss: 0.1271 - acc: 0.9608\n",
      "Epoch 31/100\n",
      "766/766 [==============================] - 0s 241us/step - loss: 0.1169 - acc: 0.9674\n",
      "Epoch 32/100\n",
      "766/766 [==============================] - 0s 338us/step - loss: 0.1111 - acc: 0.9726\n",
      "Epoch 33/100\n",
      "766/766 [==============================] - 0s 356us/step - loss: 0.1074 - acc: 0.9739\n",
      "Epoch 34/100\n",
      "766/766 [==============================] - 0s 340us/step - loss: 0.0989 - acc: 0.9778\n",
      "Epoch 35/100\n",
      "766/766 [==============================] - 0s 307us/step - loss: 0.0950 - acc: 0.9765\n",
      "Epoch 36/100\n",
      "766/766 [==============================] - 0s 258us/step - loss: 0.0893 - acc: 0.9791\n",
      "Epoch 37/100\n",
      "766/766 [==============================] - 0s 270us/step - loss: 0.0831 - acc: 0.9817\n",
      "Epoch 38/100\n",
      "766/766 [==============================] - 0s 277us/step - loss: 0.0836 - acc: 0.9804\n",
      "Epoch 39/100\n",
      "766/766 [==============================] - 0s 223us/step - loss: 0.0763 - acc: 0.9830\n",
      "Epoch 40/100\n",
      "766/766 [==============================] - 0s 196us/step - loss: 0.0743 - acc: 0.9817\n",
      "Epoch 41/100\n",
      "766/766 [==============================] - 0s 248us/step - loss: 0.0704 - acc: 0.9843\n",
      "Epoch 42/100\n",
      "766/766 [==============================] - 0s 249us/step - loss: 0.0658 - acc: 0.9843\n",
      "Epoch 43/100\n",
      "766/766 [==============================] - 0s 252us/step - loss: 0.0619 - acc: 0.9843\n",
      "Epoch 44/100\n",
      "766/766 [==============================] - 0s 230us/step - loss: 0.0588 - acc: 0.9817\n",
      "Epoch 45/100\n",
      "766/766 [==============================] - 0s 238us/step - loss: 0.0566 - acc: 0.9843\n",
      "Epoch 46/100\n",
      "766/766 [==============================] - 0s 361us/step - loss: 0.0536 - acc: 0.9856\n",
      "Epoch 47/100\n",
      "766/766 [==============================] - 0s 249us/step - loss: 0.0499 - acc: 0.9843\n",
      "Epoch 48/100\n",
      "766/766 [==============================] - 0s 206us/step - loss: 0.0478 - acc: 0.9883\n",
      "Epoch 49/100\n",
      "766/766 [==============================] - 0s 209us/step - loss: 0.0470 - acc: 0.9856\n",
      "Epoch 50/100\n",
      "766/766 [==============================] - 0s 208us/step - loss: 0.0438 - acc: 0.9856\n",
      "Epoch 51/100\n",
      "766/766 [==============================] - 0s 214us/step - loss: 0.0425 - acc: 0.9896\n",
      "Epoch 52/100\n",
      "766/766 [==============================] - 0s 213us/step - loss: 0.0387 - acc: 0.9896\n",
      "Epoch 53/100\n",
      "766/766 [==============================] - 0s 216us/step - loss: 0.0391 - acc: 0.9896\n",
      "Epoch 54/100\n",
      "766/766 [==============================] - 0s 199us/step - loss: 0.0376 - acc: 0.9883\n",
      "Epoch 55/100\n",
      "766/766 [==============================] - 0s 203us/step - loss: 0.0340 - acc: 0.9909\n",
      "Epoch 56/100\n",
      "766/766 [==============================] - 0s 236us/step - loss: 0.0327 - acc: 0.9935\n",
      "Epoch 57/100\n",
      "766/766 [==============================] - 0s 198us/step - loss: 0.0304 - acc: 0.9935\n",
      "Epoch 58/100\n",
      "766/766 [==============================] - 0s 235us/step - loss: 0.0289 - acc: 0.9935\n",
      "Epoch 59/100\n",
      "766/766 [==============================] - 0s 209us/step - loss: 0.0275 - acc: 0.9974\n",
      "Epoch 60/100\n",
      "766/766 [==============================] - 0s 225us/step - loss: 0.0259 - acc: 0.9935\n",
      "Epoch 61/100\n",
      "766/766 [==============================] - 0s 218us/step - loss: 0.0255 - acc: 0.9961\n",
      "Epoch 62/100\n",
      "766/766 [==============================] - 0s 205us/step - loss: 0.0247 - acc: 0.9948\n",
      "Epoch 63/100\n",
      "766/766 [==============================] - 0s 239us/step - loss: 0.0221 - acc: 0.9961 0s - loss: 0.0222 - acc: 0.997\n",
      "Epoch 64/100\n",
      "766/766 [==============================] - 0s 230us/step - loss: 0.0227 - acc: 0.9974\n",
      "Epoch 65/100\n",
      "766/766 [==============================] - 0s 212us/step - loss: 0.0196 - acc: 0.9974\n",
      "Epoch 66/100\n",
      "766/766 [==============================] - 0s 196us/step - loss: 0.0198 - acc: 0.9974\n",
      "Epoch 67/100\n",
      "766/766 [==============================] - 0s 226us/step - loss: 0.0201 - acc: 0.9974\n",
      "Epoch 68/100\n",
      "766/766 [==============================] - 0s 202us/step - loss: 0.0178 - acc: 0.9961\n",
      "Epoch 69/100\n",
      "766/766 [==============================] - 0s 232us/step - loss: 0.0166 - acc: 0.9974\n",
      "Epoch 70/100\n",
      "766/766 [==============================] - 0s 203us/step - loss: 0.0158 - acc: 0.9961\n",
      "Epoch 71/100\n",
      "766/766 [==============================] - 0s 200us/step - loss: 0.0133 - acc: 0.9987\n",
      "Epoch 72/100\n",
      "766/766 [==============================] - 0s 218us/step - loss: 0.0124 - acc: 0.9987\n",
      "Epoch 73/100\n",
      "766/766 [==============================] - 0s 209us/step - loss: 0.0123 - acc: 0.9987\n",
      "Epoch 74/100\n",
      "766/766 [==============================] - 0s 202us/step - loss: 0.0124 - acc: 0.9987\n",
      "Epoch 75/100\n",
      "766/766 [==============================] - 0s 216us/step - loss: 0.0105 - acc: 0.9987\n",
      "Epoch 76/100\n",
      "766/766 [==============================] - 0s 215us/step - loss: 0.0102 - acc: 0.9987\n",
      "Epoch 77/100\n",
      "766/766 [==============================] - 0s 195us/step - loss: 0.0115 - acc: 0.9987\n",
      "Epoch 78/100\n",
      "766/766 [==============================] - 0s 237us/step - loss: 0.0086 - acc: 0.9987\n",
      "Epoch 79/100\n",
      "766/766 [==============================] - 0s 205us/step - loss: 0.0087 - acc: 0.9987\n",
      "Epoch 80/100\n",
      "766/766 [==============================] - 0s 212us/step - loss: 0.0079 - acc: 0.9987\n",
      "Epoch 81/100\n",
      "766/766 [==============================] - 0s 221us/step - loss: 0.0074 - acc: 0.9987\n",
      "Epoch 82/100\n",
      "766/766 [==============================] - 0s 200us/step - loss: 0.0070 - acc: 0.9987\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 0s 232us/step - loss: 0.0064 - acc: 0.9987\n",
      "Epoch 84/100\n",
      "766/766 [==============================] - 0s 197us/step - loss: 0.0059 - acc: 0.9987\n",
      "Epoch 85/100\n",
      "766/766 [==============================] - 0s 196us/step - loss: 0.0054 - acc: 0.9987\n",
      "Epoch 86/100\n",
      "766/766 [==============================] - 0s 196us/step - loss: 0.0052 - acc: 0.9987\n",
      "Epoch 87/100\n",
      "766/766 [==============================] - 0s 219us/step - loss: 0.0048 - acc: 0.9987\n",
      "Epoch 88/100\n",
      "766/766 [==============================] - 0s 195us/step - loss: 0.0044 - acc: 0.9987\n",
      "Epoch 89/100\n",
      "766/766 [==============================] - 0s 196us/step - loss: 0.0041 - acc: 0.9987\n",
      "Epoch 90/100\n",
      "766/766 [==============================] - 0s 195us/step - loss: 0.0039 - acc: 0.9987\n",
      "Epoch 91/100\n",
      "766/766 [==============================] - 0s 196us/step - loss: 0.0036 - acc: 0.9987\n",
      "Epoch 92/100\n",
      "766/766 [==============================] - 0s 216us/step - loss: 0.0033 - acc: 0.9987\n",
      "Epoch 93/100\n",
      "766/766 [==============================] - 0s 202us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "766/766 [==============================] - 0s 195us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "766/766 [==============================] - 0s 224us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "766/766 [==============================] - 0s 202us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "766/766 [==============================] - 0s 215us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "766/766 [==============================] - 0s 198us/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "766/766 [==============================] - 0s 195us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "766/766 [==============================] - 0s 227us/step - loss: 0.0021 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a96a9b8198>"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=100,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 1s 3ms/step\n",
      "loss= 0.22566105937585235\n",
      "accuracy= 0.9427083333333334\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test,y_test)\n",
    "print('loss=',loss)\n",
    "print('accuracy=',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作者源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "766/766 [==============================] - 2s 2ms/step - loss: 0.6875 - acc: 0.6501\n",
      "Epoch 2/100\n",
      "766/766 [==============================] - 0s 199us/step - loss: 0.6546 - acc: 0.6540\n",
      "Epoch 3/100\n",
      "766/766 [==============================] - 0s 253us/step - loss: 0.6166 - acc: 0.6540\n",
      "Epoch 4/100\n",
      "766/766 [==============================] - 0s 220us/step - loss: 0.5920 - acc: 0.6540\n",
      "Epoch 5/100\n",
      "766/766 [==============================] - 0s 222us/step - loss: 0.5752 - acc: 0.6540\n",
      "Epoch 6/100\n",
      "766/766 [==============================] - 0s 213us/step - loss: 0.5648 - acc: 0.6540\n",
      "Epoch 7/100\n",
      "766/766 [==============================] - 0s 206us/step - loss: 0.5552 - acc: 0.6540\n",
      "Epoch 8/100\n",
      "766/766 [==============================] - 0s 221us/step - loss: 0.5456 - acc: 0.6540\n",
      "Epoch 9/100\n",
      "766/766 [==============================] - 0s 229us/step - loss: 0.5335 - acc: 0.7415\n",
      "Epoch 10/100\n",
      "766/766 [==============================] - 0s 210us/step - loss: 0.5160 - acc: 0.7728\n",
      "Epoch 11/100\n",
      "766/766 [==============================] - 0s 215us/step - loss: 0.4908 - acc: 0.7963\n",
      "Epoch 12/100\n",
      "766/766 [==============================] - 0s 195us/step - loss: 0.4608 - acc: 0.8081\n",
      "Epoch 13/100\n",
      "766/766 [==============================] - 0s 218us/step - loss: 0.4303 - acc: 0.8251\n",
      "Epoch 14/100\n",
      "766/766 [==============================] - 0s 196us/step - loss: 0.3921 - acc: 0.8512\n",
      "Epoch 15/100\n",
      "766/766 [==============================] - 0s 205us/step - loss: 0.3509 - acc: 0.8760\n",
      "Epoch 16/100\n",
      "766/766 [==============================] - 0s 217us/step - loss: 0.3124 - acc: 0.9191\n",
      "Epoch 17/100\n",
      "766/766 [==============================] - 0s 208us/step - loss: 0.2683 - acc: 0.9530\n",
      "Epoch 18/100\n",
      "766/766 [==============================] - 0s 204us/step - loss: 0.2320 - acc: 0.9661\n",
      "Epoch 19/100\n",
      "766/766 [==============================] - 0s 184us/step - loss: 0.1968 - acc: 0.9856\n",
      "Epoch 20/100\n",
      "766/766 [==============================] - 0s 198us/step - loss: 0.1736 - acc: 0.9856\n",
      "Epoch 21/100\n",
      "766/766 [==============================] - 0s 196us/step - loss: 0.1460 - acc: 0.9856\n",
      "Epoch 22/100\n",
      "766/766 [==============================] - 0s 218us/step - loss: 0.1287 - acc: 0.9869\n",
      "Epoch 23/100\n",
      "766/766 [==============================] - 0s 196us/step - loss: 0.1140 - acc: 0.9856\n",
      "Epoch 24/100\n",
      "766/766 [==============================] - 0s 216us/step - loss: 0.1032 - acc: 0.9869\n",
      "Epoch 25/100\n",
      "766/766 [==============================] - 0s 201us/step - loss: 0.0933 - acc: 0.9869\n",
      "Epoch 26/100\n",
      "766/766 [==============================] - 0s 211us/step - loss: 0.0870 - acc: 0.9869\n",
      "Epoch 27/100\n",
      "766/766 [==============================] - 0s 216us/step - loss: 0.0822 - acc: 0.9869\n",
      "Epoch 28/100\n",
      "766/766 [==============================] - 0s 200us/step - loss: 0.0790 - acc: 0.9869\n",
      "Epoch 29/100\n",
      "766/766 [==============================] - 0s 225us/step - loss: 0.0733 - acc: 0.9869\n",
      "Epoch 30/100\n",
      "766/766 [==============================] - 0s 217us/step - loss: 0.0699 - acc: 0.9869\n",
      "Epoch 31/100\n",
      "766/766 [==============================] - 0s 237us/step - loss: 0.0677 - acc: 0.9869\n",
      "Epoch 32/100\n",
      "766/766 [==============================] - 0s 195us/step - loss: 0.0650 - acc: 0.9869\n",
      "Epoch 33/100\n",
      "766/766 [==============================] - 0s 233us/step - loss: 0.0639 - acc: 0.9869\n",
      "Epoch 34/100\n",
      "766/766 [==============================] - 0s 213us/step - loss: 0.0621 - acc: 0.9869\n",
      "Epoch 35/100\n",
      "766/766 [==============================] - 0s 272us/step - loss: 0.0606 - acc: 0.9869\n",
      "Epoch 36/100\n",
      "766/766 [==============================] - 0s 193us/step - loss: 0.0579 - acc: 0.9869\n",
      "Epoch 37/100\n",
      "766/766 [==============================] - 0s 217us/step - loss: 0.0576 - acc: 0.9869\n",
      "Epoch 38/100\n",
      "766/766 [==============================] - 0s 273us/step - loss: 0.0561 - acc: 0.9869\n",
      "Epoch 39/100\n",
      "766/766 [==============================] - 0s 241us/step - loss: 0.0546 - acc: 0.9869\n",
      "Epoch 40/100\n",
      "766/766 [==============================] - 0s 237us/step - loss: 0.0559 - acc: 0.9869\n",
      "Epoch 41/100\n",
      "766/766 [==============================] - 0s 256us/step - loss: 0.0566 - acc: 0.9856\n",
      "Epoch 42/100\n",
      "766/766 [==============================] - 0s 325us/step - loss: 0.0533 - acc: 0.9869\n",
      "Epoch 43/100\n",
      "766/766 [==============================] - 0s 302us/step - loss: 0.0525 - acc: 0.9856\n",
      "Epoch 44/100\n",
      "766/766 [==============================] - 0s 288us/step - loss: 0.0525 - acc: 0.9856\n",
      "Epoch 45/100\n",
      "766/766 [==============================] - 0s 294us/step - loss: 0.0504 - acc: 0.9869\n",
      "Epoch 46/100\n",
      "766/766 [==============================] - 0s 361us/step - loss: 0.0538 - acc: 0.9869\n",
      "Epoch 47/100\n",
      "766/766 [==============================] - 0s 310us/step - loss: 0.0521 - acc: 0.9869\n",
      "Epoch 48/100\n",
      "766/766 [==============================] - 0s 333us/step - loss: 0.0499 - acc: 0.9869\n",
      "Epoch 49/100\n",
      "766/766 [==============================] - 0s 331us/step - loss: 0.0512 - acc: 0.9869\n",
      "Epoch 50/100\n",
      "766/766 [==============================] - 0s 212us/step - loss: 0.0494 - acc: 0.9869\n",
      "Epoch 51/100\n",
      "766/766 [==============================] - 0s 231us/step - loss: 0.0493 - acc: 0.9869\n",
      "Epoch 52/100\n",
      "766/766 [==============================] - 0s 327us/step - loss: 0.0486 - acc: 0.9869\n",
      "Epoch 53/100\n",
      "766/766 [==============================] - 0s 211us/step - loss: 0.0483 - acc: 0.9883\n",
      "Epoch 54/100\n",
      "766/766 [==============================] - 0s 327us/step - loss: 0.0479 - acc: 0.9869\n",
      "Epoch 55/100\n",
      "766/766 [==============================] - 0s 271us/step - loss: 0.0483 - acc: 0.9856\n",
      "Epoch 56/100\n",
      "766/766 [==============================] - 0s 287us/step - loss: 0.0487 - acc: 0.9856\n",
      "Epoch 57/100\n",
      "766/766 [==============================] - 0s 413us/step - loss: 0.0469 - acc: 0.9869\n",
      "Epoch 58/100\n",
      "766/766 [==============================] - 0s 285us/step - loss: 0.0476 - acc: 0.9869\n",
      "Epoch 59/100\n",
      "766/766 [==============================] - 0s 235us/step - loss: 0.0454 - acc: 0.9869\n",
      "Epoch 60/100\n",
      "766/766 [==============================] - 0s 289us/step - loss: 0.0451 - acc: 0.9869\n",
      "Epoch 61/100\n",
      "766/766 [==============================] - 0s 401us/step - loss: 0.0474 - acc: 0.9856\n",
      "Epoch 62/100\n",
      "766/766 [==============================] - 0s 434us/step - loss: 0.0480 - acc: 0.9869\n",
      "Epoch 63/100\n",
      "766/766 [==============================] - 0s 573us/step - loss: 0.0437 - acc: 0.9869\n",
      "Epoch 64/100\n",
      "766/766 [==============================] - 0s 596us/step - loss: 0.0486 - acc: 0.9843\n",
      "Epoch 65/100\n",
      "766/766 [==============================] - 0s 512us/step - loss: 0.0447 - acc: 0.9869\n",
      "Epoch 66/100\n",
      "766/766 [==============================] - 0s 434us/step - loss: 0.0470 - acc: 0.9883\n",
      "Epoch 67/100\n",
      "766/766 [==============================] - 0s 379us/step - loss: 0.0466 - acc: 0.9869\n",
      "Epoch 68/100\n",
      "766/766 [==============================] - 0s 331us/step - loss: 0.0453 - acc: 0.9883\n",
      "Epoch 69/100\n",
      "766/766 [==============================] - 0s 327us/step - loss: 0.0451 - acc: 0.9869\n",
      "Epoch 70/100\n",
      "766/766 [==============================] - 0s 227us/step - loss: 0.0447 - acc: 0.9869\n",
      "Epoch 71/100\n",
      "766/766 [==============================] - 0s 234us/step - loss: 0.0460 - acc: 0.9856\n",
      "Epoch 72/100\n",
      "766/766 [==============================] - 0s 320us/step - loss: 0.0459 - acc: 0.9869\n",
      "Epoch 73/100\n",
      "766/766 [==============================] - 0s 257us/step - loss: 0.0465 - acc: 0.9869\n",
      "Epoch 74/100\n",
      "766/766 [==============================] - 0s 325us/step - loss: 0.0442 - acc: 0.9883\n",
      "Epoch 75/100\n",
      "766/766 [==============================] - 0s 222us/step - loss: 0.0446 - acc: 0.9869\n",
      "Epoch 76/100\n",
      "766/766 [==============================] - 0s 366us/step - loss: 0.0432 - acc: 0.9869\n",
      "Epoch 77/100\n",
      "766/766 [==============================] - 0s 227us/step - loss: 0.0438 - acc: 0.9856\n",
      "Epoch 78/100\n",
      "766/766 [==============================] - 0s 237us/step - loss: 0.0443 - acc: 0.9869\n",
      "Epoch 79/100\n",
      "766/766 [==============================] - 0s 232us/step - loss: 0.0452 - acc: 0.9869\n",
      "Epoch 80/100\n",
      "766/766 [==============================] - 0s 247us/step - loss: 0.0448 - acc: 0.9869\n",
      "Epoch 81/100\n",
      "766/766 [==============================] - 0s 240us/step - loss: 0.0473 - acc: 0.9869\n",
      "Epoch 82/100\n",
      "766/766 [==============================] - 0s 271us/step - loss: 0.0436 - acc: 0.9869\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 0s 266us/step - loss: 0.0426 - acc: 0.9869\n",
      "Epoch 84/100\n",
      "766/766 [==============================] - 0s 187us/step - loss: 0.0439 - acc: 0.9869\n",
      "Epoch 85/100\n",
      "766/766 [==============================] - 0s 292us/step - loss: 0.0442 - acc: 0.9869\n",
      "Epoch 86/100\n",
      "766/766 [==============================] - 0s 411us/step - loss: 0.0444 - acc: 0.9869\n",
      "Epoch 87/100\n",
      "766/766 [==============================] - 0s 339us/step - loss: 0.0429 - acc: 0.9869\n",
      "Epoch 88/100\n",
      "766/766 [==============================] - 0s 256us/step - loss: 0.0440 - acc: 0.9869\n",
      "Epoch 89/100\n",
      "766/766 [==============================] - 0s 212us/step - loss: 0.0483 - acc: 0.9869\n",
      "Epoch 90/100\n",
      "766/766 [==============================] - 0s 209us/step - loss: 0.0434 - acc: 0.9856\n",
      "Epoch 91/100\n",
      "766/766 [==============================] - 0s 235us/step - loss: 0.0423 - acc: 0.9856\n",
      "Epoch 92/100\n",
      "766/766 [==============================] - 0s 310us/step - loss: 0.0422 - acc: 0.9869\n",
      "Epoch 93/100\n",
      "766/766 [==============================] - 0s 228us/step - loss: 0.0405 - acc: 0.9856\n",
      "Epoch 94/100\n",
      "766/766 [==============================] - 0s 234us/step - loss: 0.0444 - acc: 0.9856\n",
      "Epoch 95/100\n",
      "766/766 [==============================] - 0s 214us/step - loss: 0.0421 - acc: 0.9856\n",
      "Epoch 96/100\n",
      "766/766 [==============================] - 0s 202us/step - loss: 0.0404 - acc: 0.9869\n",
      "Epoch 97/100\n",
      "766/766 [==============================] - 0s 256us/step - loss: 0.0431 - acc: 0.9869\n",
      "Epoch 98/100\n",
      "766/766 [==============================] - 0s 251us/step - loss: 0.0417 - acc: 0.9843\n",
      "Epoch 99/100\n",
      "766/766 [==============================] - 0s 225us/step - loss: 0.0459 - acc: 0.9856\n",
      "Epoch 100/100\n",
      "766/766 [==============================] - 0s 219us/step - loss: 0.0419 - acc: 0.9869\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Import dataset\n",
    "dataset = pd.read_csv('tic_tac_toe.csv')\n",
    "X = dataset.iloc[:, 0:9].values\n",
    "y = dataset.iloc[:, 9:10].values\n",
    "\n",
    "# Encode categorical variables as numeric\n",
    "labelencoder_X = LabelEncoder()\n",
    "for _ in range(9):\n",
    "    X[:, _] = labelencoder_X.fit_transform(X[:, _])\n",
    "\n",
    "# Onehot encode all dependent categorical variables\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0,1,2,3,4,5,6,7,8])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "# Remove every third column to avoid dummy variable trap\n",
    "# Only need 2 bits to represent 3 possibilities\n",
    "X = np.delete(X, [0,3,6,9,12,15,18,21,24], axis=1)\n",
    "\n",
    "# Encode target categorical variable\n",
    "labelencoder_y = LabelEncoder()\n",
    "y[:, 0] = labelencoder_y.fit_transform(y[:, 0])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize neural network\n",
    "nnet = Sequential()\n",
    "\n",
    "# Add first hidden layer (and input layer)\n",
    "nnet.add(Dense(units=9, kernel_initializer='uniform', activation='relu', input_dim=18))\n",
    "\n",
    "# Add second hidden layer\n",
    "nnet.add(Dense(units=9, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# Add output layer\n",
    "nnet.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile network\n",
    "nnet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train network\n",
    "nnet.fit(X_train, y_train, batch_size=10, epochs=100)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = nnet.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 1s 3ms/step\n",
      "loss= 0.09561582972916464\n",
      "accuracy= 0.96875\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = nnet.evaluate(X_test,y_test)\n",
    "print('loss=',loss)\n",
    "print('accuracy=',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(y[:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
